error = 0
count = 0
for _, row in test_ds.dataset.data.items():
    s = row['sequence']
    t = row['target']
    m = 0
    for ss in s:
        m += ss[0]
    m = m / len(s)
    
    count += 1
    error += (m - t[0][0])**2

error = error / count

print(f'baseline error: {error}')


df.groupby(df.index.map(lambda x : datetime.datetime(year=x.year, month=x.month, day=x.day, hour=x.hour, minute=x.minute, second=0))).agg(['mean'])





class LinearForecaster(nn.Module):
    """ """
    def __init__(self, n_outputs: int, sequence_len: int, n_hidden: int, dropout: int = 0.2) -> None:
        super().__init__()

        layers = [
            nn.Linear(sequence_len, n_hidden),
        ]
        if dropout:
            layers.append(nn.Dropout(p=dropout))
        layers.extend([
            nn.ReLU(),
            nn.Linear(n_hidden, n_hidden),
        ])
        if dropout:
            layers.append(nn.Dropout(p=dropout))
        layers.extend([
            nn.ReLU(),
            nn.Linear(n_hidden, n_outputs),
        ])
        
        self.stack = nn.Sequential(*layers)

    def forward(self, x):
        return self.stack(x)



linear_model = LinearForecaster(nout, sequence_len, n_hidden=20).to(device)

# Set learning rate and number of epochs to train over
lr = 4e-4
n_epochs = 20

# Initialize the loss function and optimizer
criterion = nn.MSELoss().to(device)
optimizer = torch.optim.AdamW(linear_model.parameters(), lr=lr)


# Lists to store training and validation losses
t_losses, v_losses = [], []
# Loop over epochs
for epoch in range(n_epochs):
    train_loss, valid_loss = 0.0, 0.0

    # train step
    linear_model.train()
    # Loop over train dataset
    for x, y in trainloader:
        
        optimizer.zero_grad()
        # move inputs to device
        x = x.squeeze().to(device)
        y = y.squeeze().to(device)

        # Forward Pass
        preds = linear_model(x).squeeze()
        loss = criterion(preds, y)  # compute batch loss
        train_loss += loss.item()
        loss.backward()
        optimizer.step()
    epoch_loss = train_loss / len(trainloader)
    t_losses.append(epoch_loss)

    # validation step
    linear_model.eval()
    # Loop over validation dataset
    for x, y in testloader:
        with torch.no_grad():
            x, y = x.squeeze().to(device), y.squeeze().to(device)
            preds = linear_model(x).squeeze()
            error = criterion(preds, y)
        valid_loss += error.item()
    valid_loss = valid_loss / len(testloader)
    v_losses.append(valid_loss)

    print(f"{epoch} - train: {epoch_loss}, valid: {valid_loss}")

plot_losses(t_losses, v_losses)
